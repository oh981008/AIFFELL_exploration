{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트: 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import glob # 사용자가 제시한 조건에 맞는 파일명을 리스트로 반환\n",
    "import tensorflow\n",
    "\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 데이터 읽어오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\adele.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\al-green.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\alicia-keys.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\amy-winehouse.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\beatles.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bieber.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bjork.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\blink-182.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bob-dylan.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bob-marley.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\britney-spears.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bruce-springsteen.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\bruno-mars.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\cake.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\dickinson.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\disney.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\dj-khaled.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\dolly-parton.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\dr-seuss.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\drake.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\eminem.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\janisjoplin.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\jimi-hendrix.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\johnny-cash.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\joni-mitchell.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\kanye-west.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\kanye.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\Kanye_West.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\lady-gaga.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\leonard-cohen.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\lil-wayne.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\Lil_Wayne.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\lin-manuel-miranda.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\lorde.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\ludacris.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\michael-jackson.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\missy-elliott.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\nickelback.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\nicki-minaj.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\nirvana.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\notorious-big.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\notorious_big.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\nursery_rhymes.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\patti-smith.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\paul-simon.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\prince.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\r-kelly.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\radiohead.txt',\n",
       " 'C:\\\\Users\\\\오순정/aifell/node/221125_exploration_santence/lyrics\\\\rihanna.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os \n",
    "\n",
    "txt_path=os.getenv('Home')+'/aifell/node/221125_exploration_santence/lyrics/*'\n",
    "\n",
    "txt_list=glob.glob(txt_path)\n",
    "txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = [] \n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\",encoding='UTF8') as f:\n",
    "        raw = f.read().splitlines() #read() : 파일 전체의 내용을 하나의 문자열로 읽어온다. , splitlines()  : 여러라인으로 구분되어 있는 문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) # extend() : 리스트함수로 추가적인 내용을 연장 한다.\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 파일을 확인해보니 가사 특성 상 연극 대사와 달리 끝에 반복적으로 오는 문자는 없었다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for some education\n",
      "Made my way into the night\n",
      "All that bullshit conversation\n",
      "Baby, can't you read the signs? I won't bore you with the details, baby\n",
      "I don't even wanna waste your time\n",
      "Let's just say that maybe\n",
      "You could help me ease my mind\n",
      "I ain't Mr. Right But if you're looking for fast love\n",
      "If that's love in your eyes\n",
      "It's more than enough\n",
      "Had some bad love\n",
      "So fast love is all that I've got on my mind Ooh, ooh\n",
      "Ooh, ooh Looking for some affirmation\n",
      "Made my way into the sun\n",
      "My friends got their ladies\n",
      "And they're all having babies\n",
      "I just wanna have some fun I won't bore you with the details, baby\n",
      "I don't even wanna waste your time\n",
      "Let's just say that maybe\n",
      "You could help me ease my mind\n",
      "I ain't Mr. Right But if you're looking for fast love\n"
     ]
    }
   ],
   "source": [
    "# enumerate() 함수를 이용하여 raw_corpus list 내에 저장된 문장과 그 문장의 인덱스를 반환(인덱스, 문장 순)\n",
    "for idx,sentence in enumerate(raw_corpus):\n",
    "    if len(sentence)==0: continue #길이가 0인문장건너뜀\n",
    "\n",
    "    if idx> 20 : break\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 정제\n",
    "- 특수 문자 제거 \n",
    "- 모든 문자 소문자 변환 \n",
    "- 앞 뒤 <start >, <end > 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> gloria gloria gloria gloria <end>\n"
     ]
    }
   ],
   "source": [
    "import re #정규표현식 임포트\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence=sentence.lower() #소문자화\n",
    "    sentence=re.sub('[^a-zA-Z, ]', '', sentence)  # 공백, 알파벳,쉼표 제외한 특수문자 제거\n",
    "    sentence=re.sub(r'[\"  \"]+', \" \", sentence)#공백 하나 이상인 곳 공백 제거\n",
    "    sentence=sentence.strip() #양쪽 공백 제거\n",
    "    sentence=\"<start>\" +\" \"+sentence +\" \"+ \"<end>\" #앞 뒤 추가\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"    G-L-O-R-I-A (Gloria) !  G-L-O-R-I-A    Gloria   \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 구현한 함수로 정제된 문장 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> looking for some education <end>',\n",
       " '<start> made my way into the night <end>',\n",
       " '<start> all that bullshit conversation <end>',\n",
       " '<start> baby, cant you read the signs i wont bore you with the details, baby <end>',\n",
       " '<start> i dont even wanna waste your time <end>',\n",
       " '<start> lets just say that maybe <end>',\n",
       " '<start> you could help me ease my mind <end>',\n",
       " '<start> i aint mr right but if youre looking for fast love <end>',\n",
       " '<start> if thats love in your eyes <end>',\n",
       " '<start> its more than enough <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=[]\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0 : continue\n",
    "        \n",
    "    preprocessed_sentence=preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "#정제된 문장 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2 301  24 ...   0   0   0]\n",
      " [  2 212  10 ...   0   0   0]\n",
      " [  2  20  15 ...   0   0   0]\n",
      " ...\n",
      " [  2  23  69 ...   0   0   0]\n",
      " [  2  34  21 ...   0   0   0]\n",
      " [  2  23  69 ...   0   0   0]] <keras.preprocessing.text.Tokenizer object at 0x0000023FA6D870D0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000, #담을 단어 만개로 제한 \n",
    "    filters=' ', #정제하여 필터 하지 않음 \n",
    "    oov_token=\"<unk>\" #없는 단어는 엉크로 대체 \n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus) #문자 데이터를 입력 받아 리스트로 변환\n",
    "    tensor=tokenizer.texts_to_sequences(corpus) #텍스트 안의 단어들을 연속된 숫자로 변환\n",
    "    \n",
    "    \n",
    "    #토큰 갯수 15개 이상이묜 제외, <start>,<end> 포함해서 17\n",
    "    tensor_t=[]\n",
    "    \n",
    "    for i in range(len(tensor)):\n",
    "        if len(tensor[i])<=15:\n",
    "            tensor_t.append(tensor[i])\n",
    "              \n",
    "    tensor_t=tf.keras.preprocessing.sequence.pad_sequences(tensor_t,padding='post') #입력 데이터 뒤에 패딩을 붙여 시퀀스 길이 맞춤 \n",
    "    \n",
    "    print(tensor_t,tokenizer)\n",
    "    return tensor_t,tokenizer\n",
    "\n",
    "\n",
    "tensor,tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163381\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor))# tokenizer에 구축된 단어 사전 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : the\n",
      "5 : i\n",
      "6 : you\n",
      "7 : and\n",
      "8 : to\n",
      "9 : a\n",
      "10 : my\n",
      "11 : me\n",
      "12 : in\n",
      "13 : it\n",
      "14 : im\n",
      "15 : that\n",
      "16 : on\n",
      "17 : of\n",
      "18 : your\n",
      "19 : like\n",
      "20 : all\n"
     ]
    }
   ],
   "source": [
    "#단어 사전 인덱스 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx,\":\",tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx>=20 : break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start 인덱스 넘버가 2여서 모두 2로 시작함을 알 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  301   24   91 6254    3    0    0    0    0    0    0    0    0]\n",
      "[ 301   24   91 6254    3    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 패딩으로 채우진 뒷 부분 <pad> 일 가능성 높음\n",
    "src_input=tensor[:,:-1]\n",
    "#tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input=tensor[:,1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163381\n"
     ]
    }
   ],
   "source": [
    "print(len(src_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input,test_size=0.2,random_state=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train:  (130704, 14)\n",
      "Target Train:  (130704, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Source Train: ', enc_train.shape)\n",
    "print('Target Train: ', dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.인공지능 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) \n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 256 # 워드 벡터의 차원수를 말하며 단어가 추상적으로 표현되는 크기입니다.\n",
    "hidden_size = 1024 # 모델에 얼마나 많은 일꾼을 둘 것인가? 정도로 이해하면 좋다.\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) # tokenizer.num_words에 +1인 이유는 문장에 없는 pad가 사용되었기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "511/511 [==============================] - 3897s 8s/step - loss: 3.4049\n",
      "Epoch 2/10\n",
      "511/511 [==============================] - 3962s 8s/step - loss: 2.9921\n",
      "Epoch 3/10\n",
      "511/511 [==============================] - 3893s 8s/step - loss: 2.8247\n",
      "Epoch 4/10\n",
      "511/511 [==============================] - 3873s 8s/step - loss: 2.6944\n",
      "Epoch 5/10\n",
      "511/511 [==============================] - 3877s 8s/step - loss: 2.5862\n",
      "Epoch 6/10\n",
      "511/511 [==============================] - 3912s 8s/step - loss: 2.4903\n",
      "Epoch 7/10\n",
      "511/511 [==============================] - 3884s 8s/step - loss: 2.4014\n",
      "Epoch 8/10\n",
      "511/511 [==============================] - 3868s 8s/step - loss: 2.3186\n",
      "Epoch 9/10\n",
      "511/511 [==============================] - 3886s 8s/step - loss: 2.2402\n",
      "Epoch 10/10\n",
      "511/511 [==============================] - 3860s 8s/step - loss: 2.1660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23fa95bdd00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( \n",
    "    from_logits=True, reduction='none') # 클래스 분류 문제에서 softmax 함수를 거치면 from_logits = False(default값),그렇지 않으면 from_logits = True.\n",
    "\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer) # 손실함수와 훈련과정을 설정했다.\n",
    "model.fit(enc_train, dec_train, epochs=10,batch_size=256,) # 만들어둔 데이터셋으로 모델을 학습한다. 30번 학습을 반복하겠다는 의미다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  3072256   \n",
      "                                                                 \n",
      " lstm (LSTM)                 multiple                  5246976   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               multiple                  8392704   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  12301025  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장생성 함수 정의\n",
    "#모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20): #시작 문자열을 init_sentence 로 받으며 디폴트값은 <start> 를 받는다\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) #텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다 (도달 하지 못하였으면 while 루프를 돌면서 다음 단어를 예측)\n",
    "    while True: #루프를 돌면서 init_sentence에 단어를 하나씩 생성성\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4 \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated #최종적으로 모델이 생성한 문장을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love \", max_len=20)\n",
    "# generate_text 함수에 lyricist 라 정의한 모델을 이용해서 ilove 로 시작되는 문장을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i eat my <unk> <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i eat \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> fuck the fallout <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> fuck \", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she said she love me <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she \", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "욜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 validation loss가 2.16으로 2.2 이하이긴 하다.\n",
    "하지만 epoches를 10번만 돌린것이고, 파라미터를 수정도 해보지 않았다. 왜냐하면 이 모델을 학습시키는데 epoch\n",
    "한번에 1시간이 걸렸기 때문이다... 10번 반복했데 저녁 9시에 돌리기 시작해서 잠자고 9시에 일어나니까 중간에 노트북이 \n",
    "꺼졌던건지.. 여튼 거의 끝나있었다.. 시간이 너무 오래 걸려서 모델을 한번더 돌릴 엄두가 나지 않는다. \n",
    "왜 이렇게 오래 걸리지.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화 과정에서 15 이하로 줄이기 위해 함수를 계속 보니 토큰화 과정의 텐서와 패딩에 대해 더 이해가 \n",
    "잘 됬었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
